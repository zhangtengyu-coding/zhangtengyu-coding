<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>用transformers实现多输出、参数共享的bert模型 - 张胜东的博客</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="张胜东" /><meta name="description" content="背景 在nlp领域，预训练模型bert可谓是红得发紫。 但现在能搜到的大多数都是pytorch写的框架，而且大多都是单输出模型。 所以，本文以 有相" /><meta name="keywords" content="张胜东, 博客, 编程" />


<meta name="baidu-site-verification" content="qWR9jJPJ9e" />
<meta name="google-site-verification" content="s9FkJZw4X2alyC8-nsdZgiPHBwX6uqr1QVNxRaGfDKY" />


<meta name="generator" content="Hugo 0.68.3 with theme even" />


<link rel="canonical" href="https://www.zhangshengdong.com/post/multi_output_bert_model_with_transformers/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<link href="/sass/main.min.2e81bbed97b8b282c1aeb57488cc71c8d8c8ec559f3931531bd396bf31e0d4dd.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="用transformers实现多输出、参数共享的bert模型" />
<meta property="og:description" content="背景 在nlp领域，预训练模型bert可谓是红得发紫。 但现在能搜到的大多数都是pytorch写的框架，而且大多都是单输出模型。 所以，本文以 有相" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.zhangshengdong.com/post/multi_output_bert_model_with_transformers/" />
<meta property="article:published_time" content="2021-03-27T00:57:58+08:00" />
<meta property="article:modified_time" content="2021-09-09T04:13:14+08:00" />
<meta itemprop="name" content="用transformers实现多输出、参数共享的bert模型">
<meta itemprop="description" content="背景 在nlp领域，预训练模型bert可谓是红得发紫。 但现在能搜到的大多数都是pytorch写的框架，而且大多都是单输出模型。 所以，本文以 有相">
<meta itemprop="datePublished" content="2021-03-27T00:57:58&#43;08:00" />
<meta itemprop="dateModified" content="2021-09-09T04:13:14&#43;08:00" />
<meta itemprop="wordCount" content="7888">



<meta itemprop="keywords" content="深度学习,多输出,参数共享,模型,keras,bert," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="用transformers实现多输出、参数共享的bert模型"/>
<meta name="twitter:description" content="背景 在nlp领域，预训练模型bert可谓是红得发紫。 但现在能搜到的大多数都是pytorch写的框架，而且大多都是单输出模型。 所以，本文以 有相"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">张胜东的博客</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">张胜东的博客</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">用transformers实现多输出、参数共享的bert模型</h1>

      <div class="post-meta">
        <span class="post-time"> 2021-03-27 </span>
        
          <span class="more-meta"> 约 7888 字 </span>
          <span class="more-meta"> 预计阅读 16 分钟 </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次阅读 </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#背景">背景</a></li>
        <li><a href="#transformers基础应用">transformers基础应用</a></li>
        <li><a href="#附录">附录</a>
          <ul>
            <li><a href="#全部源码">全部源码</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h2 id="背景">背景</h2>
<p>在nlp领域，预训练模型bert可谓是红得发紫。</p>
<p>但现在能搜到的大多数都是pytorch写的框架，而且大多都是单输出模型。</p>
<p>所以，本文以 有相互关系的多层标签分类 为背景，用keras设计了多输出、参数共享的模型。</p>
<p>以上，是之前一篇文章的开头。那篇文章读取bert使用的是keras_bert第三方库，后来发现，兼容性等不是太好。</p>
<p>所以这篇准备采用transformers实现相同功能。（当然，兼容性要比keras_bert第三方库好一点，但限制是必须要tensorflow2）</p>
<h2 id="transformers基础应用">transformers基础应用</h2>
<p>首先，可以在命令行里，使用如下命令，将tensorflow的检查点文件转成pytorch模型文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">transformers-cli convert --model_type bert <span class="se">\
</span><span class="se"></span>  --tf_checkpoint chinese_L-12_H-768_A-12/bert_model.ckpt <span class="se">\
</span><span class="se"></span>  --config chinese_L-12_H-768_A-12/bert_config.json <span class="se">\
</span><span class="se"></span>  --pytorch_dump_output chinese_L-12_H-768_A-12/pytorch_model.bin
</code></pre></td></tr></table>
</div>
</div><p>随后，可以使用如下代码，读取bert模型：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">bert_model</span> <span class="o">=</span> <span class="n">TFBertForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&#34;./chinese_L-12_H-768_A-12/&#34;</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>最后，可以使用如下方式读取bert输出结果，并搭建模型：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">label_list</span><span class="p">):</span>
    <span class="n">K</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
    
    <span class="n">bert_model</span> <span class="o">=</span> <span class="n">TFBertForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">bert_path</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
 
    <span class="n">input_indices</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">)</span>
 
    <span class="n">bert_output</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span><span class="n">input_indices</span><span class="p">)</span>
    <span class="n">projection_logits</span> <span class="o">=</span> <span class="n">bert_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">bert_cls</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])(</span><span class="n">projection_logits</span><span class="p">)</span> <span class="c1"># 取出[CLS]对应的向量用来做分类</span>
    
    <span class="n">dropout</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">bert_cls</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label_list</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">dropout</span><span class="p">)</span>
 
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">input_indices</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span>
                  <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">),</span>    <span class="c1">#用足够小的学习率</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></td></tr></table>
</div>
</div><hr>
<h2 id="附录">附录</h2>
<h3 id="全部源码">全部源码</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">   1
</span><span class="lnt">   2
</span><span class="lnt">   3
</span><span class="lnt">   4
</span><span class="lnt">   5
</span><span class="lnt">   6
</span><span class="lnt">   7
</span><span class="lnt">   8
</span><span class="lnt">   9
</span><span class="lnt">  10
</span><span class="lnt">  11
</span><span class="lnt">  12
</span><span class="lnt">  13
</span><span class="lnt">  14
</span><span class="lnt">  15
</span><span class="lnt">  16
</span><span class="lnt">  17
</span><span class="lnt">  18
</span><span class="lnt">  19
</span><span class="lnt">  20
</span><span class="lnt">  21
</span><span class="lnt">  22
</span><span class="lnt">  23
</span><span class="lnt">  24
</span><span class="lnt">  25
</span><span class="lnt">  26
</span><span class="lnt">  27
</span><span class="lnt">  28
</span><span class="lnt">  29
</span><span class="lnt">  30
</span><span class="lnt">  31
</span><span class="lnt">  32
</span><span class="lnt">  33
</span><span class="lnt">  34
</span><span class="lnt">  35
</span><span class="lnt">  36
</span><span class="lnt">  37
</span><span class="lnt">  38
</span><span class="lnt">  39
</span><span class="lnt">  40
</span><span class="lnt">  41
</span><span class="lnt">  42
</span><span class="lnt">  43
</span><span class="lnt">  44
</span><span class="lnt">  45
</span><span class="lnt">  46
</span><span class="lnt">  47
</span><span class="lnt">  48
</span><span class="lnt">  49
</span><span class="lnt">  50
</span><span class="lnt">  51
</span><span class="lnt">  52
</span><span class="lnt">  53
</span><span class="lnt">  54
</span><span class="lnt">  55
</span><span class="lnt">  56
</span><span class="lnt">  57
</span><span class="lnt">  58
</span><span class="lnt">  59
</span><span class="lnt">  60
</span><span class="lnt">  61
</span><span class="lnt">  62
</span><span class="lnt">  63
</span><span class="lnt">  64
</span><span class="lnt">  65
</span><span class="lnt">  66
</span><span class="lnt">  67
</span><span class="lnt">  68
</span><span class="lnt">  69
</span><span class="lnt">  70
</span><span class="lnt">  71
</span><span class="lnt">  72
</span><span class="lnt">  73
</span><span class="lnt">  74
</span><span class="lnt">  75
</span><span class="lnt">  76
</span><span class="lnt">  77
</span><span class="lnt">  78
</span><span class="lnt">  79
</span><span class="lnt">  80
</span><span class="lnt">  81
</span><span class="lnt">  82
</span><span class="lnt">  83
</span><span class="lnt">  84
</span><span class="lnt">  85
</span><span class="lnt">  86
</span><span class="lnt">  87
</span><span class="lnt">  88
</span><span class="lnt">  89
</span><span class="lnt">  90
</span><span class="lnt">  91
</span><span class="lnt">  92
</span><span class="lnt">  93
</span><span class="lnt">  94
</span><span class="lnt">  95
</span><span class="lnt">  96
</span><span class="lnt">  97
</span><span class="lnt">  98
</span><span class="lnt">  99
</span><span class="lnt"> 100
</span><span class="lnt"> 101
</span><span class="lnt"> 102
</span><span class="lnt"> 103
</span><span class="lnt"> 104
</span><span class="lnt"> 105
</span><span class="lnt"> 106
</span><span class="lnt"> 107
</span><span class="lnt"> 108
</span><span class="lnt"> 109
</span><span class="lnt"> 110
</span><span class="lnt"> 111
</span><span class="lnt"> 112
</span><span class="lnt"> 113
</span><span class="lnt"> 114
</span><span class="lnt"> 115
</span><span class="lnt"> 116
</span><span class="lnt"> 117
</span><span class="lnt"> 118
</span><span class="lnt"> 119
</span><span class="lnt"> 120
</span><span class="lnt"> 121
</span><span class="lnt"> 122
</span><span class="lnt"> 123
</span><span class="lnt"> 124
</span><span class="lnt"> 125
</span><span class="lnt"> 126
</span><span class="lnt"> 127
</span><span class="lnt"> 128
</span><span class="lnt"> 129
</span><span class="lnt"> 130
</span><span class="lnt"> 131
</span><span class="lnt"> 132
</span><span class="lnt"> 133
</span><span class="lnt"> 134
</span><span class="lnt"> 135
</span><span class="lnt"> 136
</span><span class="lnt"> 137
</span><span class="lnt"> 138
</span><span class="lnt"> 139
</span><span class="lnt"> 140
</span><span class="lnt"> 141
</span><span class="lnt"> 142
</span><span class="lnt"> 143
</span><span class="lnt"> 144
</span><span class="lnt"> 145
</span><span class="lnt"> 146
</span><span class="lnt"> 147
</span><span class="lnt"> 148
</span><span class="lnt"> 149
</span><span class="lnt"> 150
</span><span class="lnt"> 151
</span><span class="lnt"> 152
</span><span class="lnt"> 153
</span><span class="lnt"> 154
</span><span class="lnt"> 155
</span><span class="lnt"> 156
</span><span class="lnt"> 157
</span><span class="lnt"> 158
</span><span class="lnt"> 159
</span><span class="lnt"> 160
</span><span class="lnt"> 161
</span><span class="lnt"> 162
</span><span class="lnt"> 163
</span><span class="lnt"> 164
</span><span class="lnt"> 165
</span><span class="lnt"> 166
</span><span class="lnt"> 167
</span><span class="lnt"> 168
</span><span class="lnt"> 169
</span><span class="lnt"> 170
</span><span class="lnt"> 171
</span><span class="lnt"> 172
</span><span class="lnt"> 173
</span><span class="lnt"> 174
</span><span class="lnt"> 175
</span><span class="lnt"> 176
</span><span class="lnt"> 177
</span><span class="lnt"> 178
</span><span class="lnt"> 179
</span><span class="lnt"> 180
</span><span class="lnt"> 181
</span><span class="lnt"> 182
</span><span class="lnt"> 183
</span><span class="lnt"> 184
</span><span class="lnt"> 185
</span><span class="lnt"> 186
</span><span class="lnt"> 187
</span><span class="lnt"> 188
</span><span class="lnt"> 189
</span><span class="lnt"> 190
</span><span class="lnt"> 191
</span><span class="lnt"> 192
</span><span class="lnt"> 193
</span><span class="lnt"> 194
</span><span class="lnt"> 195
</span><span class="lnt"> 196
</span><span class="lnt"> 197
</span><span class="lnt"> 198
</span><span class="lnt"> 199
</span><span class="lnt"> 200
</span><span class="lnt"> 201
</span><span class="lnt"> 202
</span><span class="lnt"> 203
</span><span class="lnt"> 204
</span><span class="lnt"> 205
</span><span class="lnt"> 206
</span><span class="lnt"> 207
</span><span class="lnt"> 208
</span><span class="lnt"> 209
</span><span class="lnt"> 210
</span><span class="lnt"> 211
</span><span class="lnt"> 212
</span><span class="lnt"> 213
</span><span class="lnt"> 214
</span><span class="lnt"> 215
</span><span class="lnt"> 216
</span><span class="lnt"> 217
</span><span class="lnt"> 218
</span><span class="lnt"> 219
</span><span class="lnt"> 220
</span><span class="lnt"> 221
</span><span class="lnt"> 222
</span><span class="lnt"> 223
</span><span class="lnt"> 224
</span><span class="lnt"> 225
</span><span class="lnt"> 226
</span><span class="lnt"> 227
</span><span class="lnt"> 228
</span><span class="lnt"> 229
</span><span class="lnt"> 230
</span><span class="lnt"> 231
</span><span class="lnt"> 232
</span><span class="lnt"> 233
</span><span class="lnt"> 234
</span><span class="lnt"> 235
</span><span class="lnt"> 236
</span><span class="lnt"> 237
</span><span class="lnt"> 238
</span><span class="lnt"> 239
</span><span class="lnt"> 240
</span><span class="lnt"> 241
</span><span class="lnt"> 242
</span><span class="lnt"> 243
</span><span class="lnt"> 244
</span><span class="lnt"> 245
</span><span class="lnt"> 246
</span><span class="lnt"> 247
</span><span class="lnt"> 248
</span><span class="lnt"> 249
</span><span class="lnt"> 250
</span><span class="lnt"> 251
</span><span class="lnt"> 252
</span><span class="lnt"> 253
</span><span class="lnt"> 254
</span><span class="lnt"> 255
</span><span class="lnt"> 256
</span><span class="lnt"> 257
</span><span class="lnt"> 258
</span><span class="lnt"> 259
</span><span class="lnt"> 260
</span><span class="lnt"> 261
</span><span class="lnt"> 262
</span><span class="lnt"> 263
</span><span class="lnt"> 264
</span><span class="lnt"> 265
</span><span class="lnt"> 266
</span><span class="lnt"> 267
</span><span class="lnt"> 268
</span><span class="lnt"> 269
</span><span class="lnt"> 270
</span><span class="lnt"> 271
</span><span class="lnt"> 272
</span><span class="lnt"> 273
</span><span class="lnt"> 274
</span><span class="lnt"> 275
</span><span class="lnt"> 276
</span><span class="lnt"> 277
</span><span class="lnt"> 278
</span><span class="lnt"> 279
</span><span class="lnt"> 280
</span><span class="lnt"> 281
</span><span class="lnt"> 282
</span><span class="lnt"> 283
</span><span class="lnt"> 284
</span><span class="lnt"> 285
</span><span class="lnt"> 286
</span><span class="lnt"> 287
</span><span class="lnt"> 288
</span><span class="lnt"> 289
</span><span class="lnt"> 290
</span><span class="lnt"> 291
</span><span class="lnt"> 292
</span><span class="lnt"> 293
</span><span class="lnt"> 294
</span><span class="lnt"> 295
</span><span class="lnt"> 296
</span><span class="lnt"> 297
</span><span class="lnt"> 298
</span><span class="lnt"> 299
</span><span class="lnt"> 300
</span><span class="lnt"> 301
</span><span class="lnt"> 302
</span><span class="lnt"> 303
</span><span class="lnt"> 304
</span><span class="lnt"> 305
</span><span class="lnt"> 306
</span><span class="lnt"> 307
</span><span class="lnt"> 308
</span><span class="lnt"> 309
</span><span class="lnt"> 310
</span><span class="lnt"> 311
</span><span class="lnt"> 312
</span><span class="lnt"> 313
</span><span class="lnt"> 314
</span><span class="lnt"> 315
</span><span class="lnt"> 316
</span><span class="lnt"> 317
</span><span class="lnt"> 318
</span><span class="lnt"> 319
</span><span class="lnt"> 320
</span><span class="lnt"> 321
</span><span class="lnt"> 322
</span><span class="lnt"> 323
</span><span class="lnt"> 324
</span><span class="lnt"> 325
</span><span class="lnt"> 326
</span><span class="lnt"> 327
</span><span class="lnt"> 328
</span><span class="lnt"> 329
</span><span class="lnt"> 330
</span><span class="lnt"> 331
</span><span class="lnt"> 332
</span><span class="lnt"> 333
</span><span class="lnt"> 334
</span><span class="lnt"> 335
</span><span class="lnt"> 336
</span><span class="lnt"> 337
</span><span class="lnt"> 338
</span><span class="lnt"> 339
</span><span class="lnt"> 340
</span><span class="lnt"> 341
</span><span class="lnt"> 342
</span><span class="lnt"> 343
</span><span class="lnt"> 344
</span><span class="lnt"> 345
</span><span class="lnt"> 346
</span><span class="lnt"> 347
</span><span class="lnt"> 348
</span><span class="lnt"> 349
</span><span class="lnt"> 350
</span><span class="lnt"> 351
</span><span class="lnt"> 352
</span><span class="lnt"> 353
</span><span class="lnt"> 354
</span><span class="lnt"> 355
</span><span class="lnt"> 356
</span><span class="lnt"> 357
</span><span class="lnt"> 358
</span><span class="lnt"> 359
</span><span class="lnt"> 360
</span><span class="lnt"> 361
</span><span class="lnt"> 362
</span><span class="lnt"> 363
</span><span class="lnt"> 364
</span><span class="lnt"> 365
</span><span class="lnt"> 366
</span><span class="lnt"> 367
</span><span class="lnt"> 368
</span><span class="lnt"> 369
</span><span class="lnt"> 370
</span><span class="lnt"> 371
</span><span class="lnt"> 372
</span><span class="lnt"> 373
</span><span class="lnt"> 374
</span><span class="lnt"> 375
</span><span class="lnt"> 376
</span><span class="lnt"> 377
</span><span class="lnt"> 378
</span><span class="lnt"> 379
</span><span class="lnt"> 380
</span><span class="lnt"> 381
</span><span class="lnt"> 382
</span><span class="lnt"> 383
</span><span class="lnt"> 384
</span><span class="lnt"> 385
</span><span class="lnt"> 386
</span><span class="lnt"> 387
</span><span class="lnt"> 388
</span><span class="lnt"> 389
</span><span class="lnt"> 390
</span><span class="lnt"> 391
</span><span class="lnt"> 392
</span><span class="lnt"> 393
</span><span class="lnt"> 394
</span><span class="lnt"> 395
</span><span class="lnt"> 396
</span><span class="lnt"> 397
</span><span class="lnt"> 398
</span><span class="lnt"> 399
</span><span class="lnt"> 400
</span><span class="lnt"> 401
</span><span class="lnt"> 402
</span><span class="lnt"> 403
</span><span class="lnt"> 404
</span><span class="lnt"> 405
</span><span class="lnt"> 406
</span><span class="lnt"> 407
</span><span class="lnt"> 408
</span><span class="lnt"> 409
</span><span class="lnt"> 410
</span><span class="lnt"> 411
</span><span class="lnt"> 412
</span><span class="lnt"> 413
</span><span class="lnt"> 414
</span><span class="lnt"> 415
</span><span class="lnt"> 416
</span><span class="lnt"> 417
</span><span class="lnt"> 418
</span><span class="lnt"> 419
</span><span class="lnt"> 420
</span><span class="lnt"> 421
</span><span class="lnt"> 422
</span><span class="lnt"> 423
</span><span class="lnt"> 424
</span><span class="lnt"> 425
</span><span class="lnt"> 426
</span><span class="lnt"> 427
</span><span class="lnt"> 428
</span><span class="lnt"> 429
</span><span class="lnt"> 430
</span><span class="lnt"> 431
</span><span class="lnt"> 432
</span><span class="lnt"> 433
</span><span class="lnt"> 434
</span><span class="lnt"> 435
</span><span class="lnt"> 436
</span><span class="lnt"> 437
</span><span class="lnt"> 438
</span><span class="lnt"> 439
</span><span class="lnt"> 440
</span><span class="lnt"> 441
</span><span class="lnt"> 442
</span><span class="lnt"> 443
</span><span class="lnt"> 444
</span><span class="lnt"> 445
</span><span class="lnt"> 446
</span><span class="lnt"> 447
</span><span class="lnt"> 448
</span><span class="lnt"> 449
</span><span class="lnt"> 450
</span><span class="lnt"> 451
</span><span class="lnt"> 452
</span><span class="lnt"> 453
</span><span class="lnt"> 454
</span><span class="lnt"> 455
</span><span class="lnt"> 456
</span><span class="lnt"> 457
</span><span class="lnt"> 458
</span><span class="lnt"> 459
</span><span class="lnt"> 460
</span><span class="lnt"> 461
</span><span class="lnt"> 462
</span><span class="lnt"> 463
</span><span class="lnt"> 464
</span><span class="lnt"> 465
</span><span class="lnt"> 466
</span><span class="lnt"> 467
</span><span class="lnt"> 468
</span><span class="lnt"> 469
</span><span class="lnt"> 470
</span><span class="lnt"> 471
</span><span class="lnt"> 472
</span><span class="lnt"> 473
</span><span class="lnt"> 474
</span><span class="lnt"> 475
</span><span class="lnt"> 476
</span><span class="lnt"> 477
</span><span class="lnt"> 478
</span><span class="lnt"> 479
</span><span class="lnt"> 480
</span><span class="lnt"> 481
</span><span class="lnt"> 482
</span><span class="lnt"> 483
</span><span class="lnt"> 484
</span><span class="lnt"> 485
</span><span class="lnt"> 486
</span><span class="lnt"> 487
</span><span class="lnt"> 488
</span><span class="lnt"> 489
</span><span class="lnt"> 490
</span><span class="lnt"> 491
</span><span class="lnt"> 492
</span><span class="lnt"> 493
</span><span class="lnt"> 494
</span><span class="lnt"> 495
</span><span class="lnt"> 496
</span><span class="lnt"> 497
</span><span class="lnt"> 498
</span><span class="lnt"> 499
</span><span class="lnt"> 500
</span><span class="lnt"> 501
</span><span class="lnt"> 502
</span><span class="lnt"> 503
</span><span class="lnt"> 504
</span><span class="lnt"> 505
</span><span class="lnt"> 506
</span><span class="lnt"> 507
</span><span class="lnt"> 508
</span><span class="lnt"> 509
</span><span class="lnt"> 510
</span><span class="lnt"> 511
</span><span class="lnt"> 512
</span><span class="lnt"> 513
</span><span class="lnt"> 514
</span><span class="lnt"> 515
</span><span class="lnt"> 516
</span><span class="lnt"> 517
</span><span class="lnt"> 518
</span><span class="lnt"> 519
</span><span class="lnt"> 520
</span><span class="lnt"> 521
</span><span class="lnt"> 522
</span><span class="lnt"> 523
</span><span class="lnt"> 524
</span><span class="lnt"> 525
</span><span class="lnt"> 526
</span><span class="lnt"> 527
</span><span class="lnt"> 528
</span><span class="lnt"> 529
</span><span class="lnt"> 530
</span><span class="lnt"> 531
</span><span class="lnt"> 532
</span><span class="lnt"> 533
</span><span class="lnt"> 534
</span><span class="lnt"> 535
</span><span class="lnt"> 536
</span><span class="lnt"> 537
</span><span class="lnt"> 538
</span><span class="lnt"> 539
</span><span class="lnt"> 540
</span><span class="lnt"> 541
</span><span class="lnt"> 542
</span><span class="lnt"> 543
</span><span class="lnt"> 544
</span><span class="lnt"> 545
</span><span class="lnt"> 546
</span><span class="lnt"> 547
</span><span class="lnt"> 548
</span><span class="lnt"> 549
</span><span class="lnt"> 550
</span><span class="lnt"> 551
</span><span class="lnt"> 552
</span><span class="lnt"> 553
</span><span class="lnt"> 554
</span><span class="lnt"> 555
</span><span class="lnt"> 556
</span><span class="lnt"> 557
</span><span class="lnt"> 558
</span><span class="lnt"> 559
</span><span class="lnt"> 560
</span><span class="lnt"> 561
</span><span class="lnt"> 562
</span><span class="lnt"> 563
</span><span class="lnt"> 564
</span><span class="lnt"> 565
</span><span class="lnt"> 566
</span><span class="lnt"> 567
</span><span class="lnt"> 568
</span><span class="lnt"> 569
</span><span class="lnt"> 570
</span><span class="lnt"> 571
</span><span class="lnt"> 572
</span><span class="lnt"> 573
</span><span class="lnt"> 574
</span><span class="lnt"> 575
</span><span class="lnt"> 576
</span><span class="lnt"> 577
</span><span class="lnt"> 578
</span><span class="lnt"> 579
</span><span class="lnt"> 580
</span><span class="lnt"> 581
</span><span class="lnt"> 582
</span><span class="lnt"> 583
</span><span class="lnt"> 584
</span><span class="lnt"> 585
</span><span class="lnt"> 586
</span><span class="lnt"> 587
</span><span class="lnt"> 588
</span><span class="lnt"> 589
</span><span class="lnt"> 590
</span><span class="lnt"> 591
</span><span class="lnt"> 592
</span><span class="lnt"> 593
</span><span class="lnt"> 594
</span><span class="lnt"> 595
</span><span class="lnt"> 596
</span><span class="lnt"> 597
</span><span class="lnt"> 598
</span><span class="lnt"> 599
</span><span class="lnt"> 600
</span><span class="lnt"> 601
</span><span class="lnt"> 602
</span><span class="lnt"> 603
</span><span class="lnt"> 604
</span><span class="lnt"> 605
</span><span class="lnt"> 606
</span><span class="lnt"> 607
</span><span class="lnt"> 608
</span><span class="lnt"> 609
</span><span class="lnt"> 610
</span><span class="lnt"> 611
</span><span class="lnt"> 612
</span><span class="lnt"> 613
</span><span class="lnt"> 614
</span><span class="lnt"> 615
</span><span class="lnt"> 616
</span><span class="lnt"> 617
</span><span class="lnt"> 618
</span><span class="lnt"> 619
</span><span class="lnt"> 620
</span><span class="lnt"> 621
</span><span class="lnt"> 622
</span><span class="lnt"> 623
</span><span class="lnt"> 624
</span><span class="lnt"> 625
</span><span class="lnt"> 626
</span><span class="lnt"> 627
</span><span class="lnt"> 628
</span><span class="lnt"> 629
</span><span class="lnt"> 630
</span><span class="lnt"> 631
</span><span class="lnt"> 632
</span><span class="lnt"> 633
</span><span class="lnt"> 634
</span><span class="lnt"> 635
</span><span class="lnt"> 636
</span><span class="lnt"> 637
</span><span class="lnt"> 638
</span><span class="lnt"> 639
</span><span class="lnt"> 640
</span><span class="lnt"> 641
</span><span class="lnt"> 642
</span><span class="lnt"> 643
</span><span class="lnt"> 644
</span><span class="lnt"> 645
</span><span class="lnt"> 646
</span><span class="lnt"> 647
</span><span class="lnt"> 648
</span><span class="lnt"> 649
</span><span class="lnt"> 650
</span><span class="lnt"> 651
</span><span class="lnt"> 652
</span><span class="lnt"> 653
</span><span class="lnt"> 654
</span><span class="lnt"> 655
</span><span class="lnt"> 656
</span><span class="lnt"> 657
</span><span class="lnt"> 658
</span><span class="lnt"> 659
</span><span class="lnt"> 660
</span><span class="lnt"> 661
</span><span class="lnt"> 662
</span><span class="lnt"> 663
</span><span class="lnt"> 664
</span><span class="lnt"> 665
</span><span class="lnt"> 666
</span><span class="lnt"> 667
</span><span class="lnt"> 668
</span><span class="lnt"> 669
</span><span class="lnt"> 670
</span><span class="lnt"> 671
</span><span class="lnt"> 672
</span><span class="lnt"> 673
</span><span class="lnt"> 674
</span><span class="lnt"> 675
</span><span class="lnt"> 676
</span><span class="lnt"> 677
</span><span class="lnt"> 678
</span><span class="lnt"> 679
</span><span class="lnt"> 680
</span><span class="lnt"> 681
</span><span class="lnt"> 682
</span><span class="lnt"> 683
</span><span class="lnt"> 684
</span><span class="lnt"> 685
</span><span class="lnt"> 686
</span><span class="lnt"> 687
</span><span class="lnt"> 688
</span><span class="lnt"> 689
</span><span class="lnt"> 690
</span><span class="lnt"> 691
</span><span class="lnt"> 692
</span><span class="lnt"> 693
</span><span class="lnt"> 694
</span><span class="lnt"> 695
</span><span class="lnt"> 696
</span><span class="lnt"> 697
</span><span class="lnt"> 698
</span><span class="lnt"> 699
</span><span class="lnt"> 700
</span><span class="lnt"> 701
</span><span class="lnt"> 702
</span><span class="lnt"> 703
</span><span class="lnt"> 704
</span><span class="lnt"> 705
</span><span class="lnt"> 706
</span><span class="lnt"> 707
</span><span class="lnt"> 708
</span><span class="lnt"> 709
</span><span class="lnt"> 710
</span><span class="lnt"> 711
</span><span class="lnt"> 712
</span><span class="lnt"> 713
</span><span class="lnt"> 714
</span><span class="lnt"> 715
</span><span class="lnt"> 716
</span><span class="lnt"> 717
</span><span class="lnt"> 718
</span><span class="lnt"> 719
</span><span class="lnt"> 720
</span><span class="lnt"> 721
</span><span class="lnt"> 722
</span><span class="lnt"> 723
</span><span class="lnt"> 724
</span><span class="lnt"> 725
</span><span class="lnt"> 726
</span><span class="lnt"> 727
</span><span class="lnt"> 728
</span><span class="lnt"> 729
</span><span class="lnt"> 730
</span><span class="lnt"> 731
</span><span class="lnt"> 732
</span><span class="lnt"> 733
</span><span class="lnt"> 734
</span><span class="lnt"> 735
</span><span class="lnt"> 736
</span><span class="lnt"> 737
</span><span class="lnt"> 738
</span><span class="lnt"> 739
</span><span class="lnt"> 740
</span><span class="lnt"> 741
</span><span class="lnt"> 742
</span><span class="lnt"> 743
</span><span class="lnt"> 744
</span><span class="lnt"> 745
</span><span class="lnt"> 746
</span><span class="lnt"> 747
</span><span class="lnt"> 748
</span><span class="lnt"> 749
</span><span class="lnt"> 750
</span><span class="lnt"> 751
</span><span class="lnt"> 752
</span><span class="lnt"> 753
</span><span class="lnt"> 754
</span><span class="lnt"> 755
</span><span class="lnt"> 756
</span><span class="lnt"> 757
</span><span class="lnt"> 758
</span><span class="lnt"> 759
</span><span class="lnt"> 760
</span><span class="lnt"> 761
</span><span class="lnt"> 762
</span><span class="lnt"> 763
</span><span class="lnt"> 764
</span><span class="lnt"> 765
</span><span class="lnt"> 766
</span><span class="lnt"> 767
</span><span class="lnt"> 768
</span><span class="lnt"> 769
</span><span class="lnt"> 770
</span><span class="lnt"> 771
</span><span class="lnt"> 772
</span><span class="lnt"> 773
</span><span class="lnt"> 774
</span><span class="lnt"> 775
</span><span class="lnt"> 776
</span><span class="lnt"> 777
</span><span class="lnt"> 778
</span><span class="lnt"> 779
</span><span class="lnt"> 780
</span><span class="lnt"> 781
</span><span class="lnt"> 782
</span><span class="lnt"> 783
</span><span class="lnt"> 784
</span><span class="lnt"> 785
</span><span class="lnt"> 786
</span><span class="lnt"> 787
</span><span class="lnt"> 788
</span><span class="lnt"> 789
</span><span class="lnt"> 790
</span><span class="lnt"> 791
</span><span class="lnt"> 792
</span><span class="lnt"> 793
</span><span class="lnt"> 794
</span><span class="lnt"> 795
</span><span class="lnt"> 796
</span><span class="lnt"> 797
</span><span class="lnt"> 798
</span><span class="lnt"> 799
</span><span class="lnt"> 800
</span><span class="lnt"> 801
</span><span class="lnt"> 802
</span><span class="lnt"> 803
</span><span class="lnt"> 804
</span><span class="lnt"> 805
</span><span class="lnt"> 806
</span><span class="lnt"> 807
</span><span class="lnt"> 808
</span><span class="lnt"> 809
</span><span class="lnt"> 810
</span><span class="lnt"> 811
</span><span class="lnt"> 812
</span><span class="lnt"> 813
</span><span class="lnt"> 814
</span><span class="lnt"> 815
</span><span class="lnt"> 816
</span><span class="lnt"> 817
</span><span class="lnt"> 818
</span><span class="lnt"> 819
</span><span class="lnt"> 820
</span><span class="lnt"> 821
</span><span class="lnt"> 822
</span><span class="lnt"> 823
</span><span class="lnt"> 824
</span><span class="lnt"> 825
</span><span class="lnt"> 826
</span><span class="lnt"> 827
</span><span class="lnt"> 828
</span><span class="lnt"> 829
</span><span class="lnt"> 830
</span><span class="lnt"> 831
</span><span class="lnt"> 832
</span><span class="lnt"> 833
</span><span class="lnt"> 834
</span><span class="lnt"> 835
</span><span class="lnt"> 836
</span><span class="lnt"> 837
</span><span class="lnt"> 838
</span><span class="lnt"> 839
</span><span class="lnt"> 840
</span><span class="lnt"> 841
</span><span class="lnt"> 842
</span><span class="lnt"> 843
</span><span class="lnt"> 844
</span><span class="lnt"> 845
</span><span class="lnt"> 846
</span><span class="lnt"> 847
</span><span class="lnt"> 848
</span><span class="lnt"> 849
</span><span class="lnt"> 850
</span><span class="lnt"> 851
</span><span class="lnt"> 852
</span><span class="lnt"> 853
</span><span class="lnt"> 854
</span><span class="lnt"> 855
</span><span class="lnt"> 856
</span><span class="lnt"> 857
</span><span class="lnt"> 858
</span><span class="lnt"> 859
</span><span class="lnt"> 860
</span><span class="lnt"> 861
</span><span class="lnt"> 862
</span><span class="lnt"> 863
</span><span class="lnt"> 864
</span><span class="lnt"> 865
</span><span class="lnt"> 866
</span><span class="lnt"> 867
</span><span class="lnt"> 868
</span><span class="lnt"> 869
</span><span class="lnt"> 870
</span><span class="lnt"> 871
</span><span class="lnt"> 872
</span><span class="lnt"> 873
</span><span class="lnt"> 874
</span><span class="lnt"> 875
</span><span class="lnt"> 876
</span><span class="lnt"> 877
</span><span class="lnt"> 878
</span><span class="lnt"> 879
</span><span class="lnt"> 880
</span><span class="lnt"> 881
</span><span class="lnt"> 882
</span><span class="lnt"> 883
</span><span class="lnt"> 884
</span><span class="lnt"> 885
</span><span class="lnt"> 886
</span><span class="lnt"> 887
</span><span class="lnt"> 888
</span><span class="lnt"> 889
</span><span class="lnt"> 890
</span><span class="lnt"> 891
</span><span class="lnt"> 892
</span><span class="lnt"> 893
</span><span class="lnt"> 894
</span><span class="lnt"> 895
</span><span class="lnt"> 896
</span><span class="lnt"> 897
</span><span class="lnt"> 898
</span><span class="lnt"> 899
</span><span class="lnt"> 900
</span><span class="lnt"> 901
</span><span class="lnt"> 902
</span><span class="lnt"> 903
</span><span class="lnt"> 904
</span><span class="lnt"> 905
</span><span class="lnt"> 906
</span><span class="lnt"> 907
</span><span class="lnt"> 908
</span><span class="lnt"> 909
</span><span class="lnt"> 910
</span><span class="lnt"> 911
</span><span class="lnt"> 912
</span><span class="lnt"> 913
</span><span class="lnt"> 914
</span><span class="lnt"> 915
</span><span class="lnt"> 916
</span><span class="lnt"> 917
</span><span class="lnt"> 918
</span><span class="lnt"> 919
</span><span class="lnt"> 920
</span><span class="lnt"> 921
</span><span class="lnt"> 922
</span><span class="lnt"> 923
</span><span class="lnt"> 924
</span><span class="lnt"> 925
</span><span class="lnt"> 926
</span><span class="lnt"> 927
</span><span class="lnt"> 928
</span><span class="lnt"> 929
</span><span class="lnt"> 930
</span><span class="lnt"> 931
</span><span class="lnt"> 932
</span><span class="lnt"> 933
</span><span class="lnt"> 934
</span><span class="lnt"> 935
</span><span class="lnt"> 936
</span><span class="lnt"> 937
</span><span class="lnt"> 938
</span><span class="lnt"> 939
</span><span class="lnt"> 940
</span><span class="lnt"> 941
</span><span class="lnt"> 942
</span><span class="lnt"> 943
</span><span class="lnt"> 944
</span><span class="lnt"> 945
</span><span class="lnt"> 946
</span><span class="lnt"> 947
</span><span class="lnt"> 948
</span><span class="lnt"> 949
</span><span class="lnt"> 950
</span><span class="lnt"> 951
</span><span class="lnt"> 952
</span><span class="lnt"> 953
</span><span class="lnt"> 954
</span><span class="lnt"> 955
</span><span class="lnt"> 956
</span><span class="lnt"> 957
</span><span class="lnt"> 958
</span><span class="lnt"> 959
</span><span class="lnt"> 960
</span><span class="lnt"> 961
</span><span class="lnt"> 962
</span><span class="lnt"> 963
</span><span class="lnt"> 964
</span><span class="lnt"> 965
</span><span class="lnt"> 966
</span><span class="lnt"> 967
</span><span class="lnt"> 968
</span><span class="lnt"> 969
</span><span class="lnt"> 970
</span><span class="lnt"> 971
</span><span class="lnt"> 972
</span><span class="lnt"> 973
</span><span class="lnt"> 974
</span><span class="lnt"> 975
</span><span class="lnt"> 976
</span><span class="lnt"> 977
</span><span class="lnt"> 978
</span><span class="lnt"> 979
</span><span class="lnt"> 980
</span><span class="lnt"> 981
</span><span class="lnt"> 982
</span><span class="lnt"> 983
</span><span class="lnt"> 984
</span><span class="lnt"> 985
</span><span class="lnt"> 986
</span><span class="lnt"> 987
</span><span class="lnt"> 988
</span><span class="lnt"> 989
</span><span class="lnt"> 990
</span><span class="lnt"> 991
</span><span class="lnt"> 992
</span><span class="lnt"> 993
</span><span class="lnt"> 994
</span><span class="lnt"> 995
</span><span class="lnt"> 996
</span><span class="lnt"> 997
</span><span class="lnt"> 998
</span><span class="lnt"> 999
</span><span class="lnt">1000
</span><span class="lnt">1001
</span><span class="lnt">1002
</span><span class="lnt">1003
</span><span class="lnt">1004
</span><span class="lnt">1005
</span><span class="lnt">1006
</span><span class="lnt">1007
</span><span class="lnt">1008
</span><span class="lnt">1009
</span><span class="lnt">1010
</span><span class="lnt">1011
</span><span class="lnt">1012
</span><span class="lnt">1013
</span><span class="lnt">1014
</span><span class="lnt">1015
</span><span class="lnt">1016
</span><span class="lnt">1017
</span><span class="lnt">1018
</span><span class="lnt">1019
</span><span class="lnt">1020
</span><span class="lnt">1021
</span><span class="lnt">1022
</span><span class="lnt">1023
</span><span class="lnt">1024
</span><span class="lnt">1025
</span><span class="lnt">1026
</span><span class="lnt">1027
</span><span class="lnt">1028
</span><span class="lnt">1029
</span><span class="lnt">1030
</span><span class="lnt">1031
</span><span class="lnt">1032
</span><span class="lnt">1033
</span><span class="lnt">1034
</span><span class="lnt">1035
</span><span class="lnt">1036
</span><span class="lnt">1037
</span><span class="lnt">1038
</span><span class="lnt">1039
</span><span class="lnt">1040
</span><span class="lnt">1041
</span><span class="lnt">1042
</span><span class="lnt">1043
</span><span class="lnt">1044
</span><span class="lnt">1045
</span><span class="lnt">1046
</span><span class="lnt">1047
</span><span class="lnt">1048
</span><span class="lnt">1049
</span><span class="lnt">1050
</span><span class="lnt">1051
</span><span class="lnt">1052
</span><span class="lnt">1053
</span><span class="lnt">1054
</span><span class="lnt">1055
</span><span class="lnt">1056
</span><span class="lnt">1057
</span><span class="lnt">1058
</span><span class="lnt">1059
</span><span class="lnt">1060
</span><span class="lnt">1061
</span><span class="lnt">1062
</span><span class="lnt">1063
</span><span class="lnt">1064
</span><span class="lnt">1065
</span><span class="lnt">1066
</span><span class="lnt">1067
</span><span class="lnt">1068
</span><span class="lnt">1069
</span><span class="lnt">1070
</span><span class="lnt">1071
</span><span class="lnt">1072
</span><span class="lnt">1073
</span><span class="lnt">1074
</span><span class="lnt">1075
</span><span class="lnt">1076
</span><span class="lnt">1077
</span><span class="lnt">1078
</span><span class="lnt">1079
</span><span class="lnt">1080
</span><span class="lnt">1081
</span><span class="lnt">1082
</span><span class="lnt">1083
</span><span class="lnt">1084
</span><span class="lnt">1085
</span><span class="lnt">1086
</span><span class="lnt">1087
</span><span class="lnt">1088
</span><span class="lnt">1089
</span><span class="lnt">1090
</span><span class="lnt">1091
</span><span class="lnt">1092
</span><span class="lnt">1093
</span><span class="lnt">1094
</span><span class="lnt">1095
</span><span class="lnt">1096
</span><span class="lnt">1097
</span><span class="lnt">1098
</span><span class="lnt">1099
</span><span class="lnt">1100
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-markdown" data-lang="markdown"><span class="gh"># 导包
</span><span class="gh"></span>

<span class="s">```python
</span><span class="s"></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras</span> <span class="kn">as</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">keras.metrics</span> <span class="kn">import</span> <span class="n">top_k_categorical_accuracy</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">load_model</span>
<span class="kn">import</span> <span class="nn">keras.backend</span> <span class="kn">as</span> <span class="nn">K</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">BertTokenizer</span><span class="p">,</span>
    <span class="n">TFBertForPreTraining</span><span class="p">,</span>
    <span class="n">TFBertModel</span><span class="p">,</span>
<span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">data_path</span> <span class="o">=</span> <span class="s2">&#34;000_text_classifier_tensorflow_textcnn/THUCNews/&#34;</span>
<span class="n">text_max_length</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">bert_path</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;chinese_L-12_H-768_A-12&#34;</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>

<span class="gh"># 构建原数据文本迭代器
</span><span class="gh"></span>

<span class="s">```python
</span><span class="s"></span><span class="k">def</span> <span class="nf">_read_file</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;读取一个文件并转换为一行&#34;&#34;&#34;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;。&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\u3000</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;。+&#39;</span><span class="p">,</span> <span class="s1">&#39;。&#39;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="k">def</span> <span class="nf">get_data_iterator</span><span class="p">(</span><span class="n">data_path</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">category</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">data_path</span><span class="p">):</span>
        <span class="n">category_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">category</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">category_path</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">_read_file</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">category_path</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)),</span> <span class="n">category</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">it</span> <span class="o">=</span> <span class="n">get_data_iterator</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="nb">next</span><span class="p">(</span><span class="n">it</span><span class="p">)</span>
<span class="s">```</span>




    (&#39;竞彩解析：日本美国争冠死磕 两巴相逢必有生死。周日受注赛事，女足世界杯决赛、美洲杯两场1/4决赛毫无疑问是全世界球迷和彩民关注的焦点。本届女足世界杯的最大黑马日本队能否一黑到底，创造亚洲奇迹？女子足坛霸主美国队能否再次“灭黑”成功，成就三冠伟业？巴西、巴拉圭冤家路窄，谁又能笑到最后？诸多谜底，在周一凌晨就会揭晓。日本美国争冠死磕。本届女足世界杯，是颠覆与反颠覆之争。夺冠大热门东道主德国队1/4决赛被日本队加时赛一球而“黑”，另一个夺冠大热门瑞典队则在半决赛被日本队3:1彻底打垮。而美国队则捍卫着女足豪强的尊严，在1/4决赛，她们与巴西女足苦战至点球大战，最终以5:3淘汰这支迅速崛起的黑马球队，而在半决赛，她们更是3:1大胜欧洲黑马法国队。美日两队此次世界杯进程惊人相似，小组赛前两轮全胜，最后一轮输球，1/4决赛同样与对手90分钟内战成平局，半决赛竟同样3:1大胜对手。此次决战，无论是日本还是美国队夺冠，均将创造女足世界杯新的历史。两巴相逢必有生死。本届美洲杯，让人大跌眼镜的事情太多。巴西、巴拉圭冤家路窄似乎更具传奇色彩。两队小组赛同分在B组，原本两个出线大热门，却双双在前两轮小组赛战平，两队直接交锋就是2:2平局，结果双双面临出局危险。最后一轮，巴西队在下半场终于发威，4:2大胜厄瓜多尔后来居上以小组第一出线，而巴拉圭最后一战还是3:3战平委内瑞拉获得小组第三，侥幸凭借净胜球优势挤掉A组第三名的哥斯达黎加，获得一个八强席位。在小组赛，巴西队是在最后时刻才逼平了巴拉圭，他们的好运气会在淘汰赛再显神威吗？巴拉圭此前3轮小组赛似乎都缺乏运气，此番又会否被幸运之神补偿一下呢？。另一场美洲杯1/4决赛，智利队在C组小组赛2胜1平以小组头名晋级八强；而委内瑞拉在B组是最不被看好的球队，但竟然在与巴西、巴拉圭同组的情况下，前两轮就奠定了小组出线权，他们小组3战1胜2平保持不败战绩，而入球数跟智利一样都是4球，只是失球数比智利多了1个。但既然他们面对强大的巴西都能保持球门不失，此番再创佳绩也不足为怪。&#39;,
     &#39;彩票&#39;)




<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>

<span class="gh"># 构建标签表
</span><span class="gh"></span>

<span class="s">```python
</span><span class="s"></span><span class="k">def</span> <span class="nf">read_category</span><span class="p">(</span><span class="n">data_path</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;读取分类目录，固定&#34;&#34;&#34;</span>
    <span class="n">categories</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>

    <span class="n">cat_to_id</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">categories</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">categories</span><span class="p">))))</span>

    <span class="k">return</span> <span class="n">categories</span><span class="p">,</span> <span class="n">cat_to_id</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">categories</span><span class="p">,</span> <span class="n">cat_to_id</span> <span class="o">=</span> <span class="n">read_category</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">cat_to_id</span>
<span class="s">```</span>




    {&#39;彩票&#39;: 0,
     &#39;家居&#39;: 1,
     &#39;游戏&#39;: 2,
     &#39;股票&#39;: 3,
     &#39;科技&#39;: 4,
     &#39;社会&#39;: 5,
     &#39;财经&#39;: 6,
     &#39;时尚&#39;: 7,
     &#39;星座&#39;: 8,
     &#39;体育&#39;: 9,
     &#39;房产&#39;: 10,
     &#39;娱乐&#39;: 11,
     &#39;时政&#39;: 12,
     &#39;教育&#39;: 13}




<span class="s">```python
</span><span class="s"></span><span class="n">categories</span>
<span class="s">```</span>




    [&#39;彩票&#39;,
     &#39;家居&#39;,
     &#39;游戏&#39;,
     &#39;股票&#39;,
     &#39;科技&#39;,
     &#39;社会&#39;,
     &#39;财经&#39;,
     &#39;时尚&#39;,
     &#39;星座&#39;,
     &#39;体育&#39;,
     &#39;房产&#39;,
     &#39;娱乐&#39;,
     &#39;时政&#39;,
     &#39;教育&#39;]




<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>

<span class="gh"># 构建训练、验证、测试集
</span><span class="gh"></span>

<span class="s">```python
</span><span class="s"></span><span class="k">def</span> <span class="nf">build_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">train_path</span><span class="p">,</span> <span class="n">dev_path</span><span class="p">,</span> <span class="n">test_path</span><span class="p">):</span>
    <span class="n">data_iter</span> <span class="o">=</span> <span class="n">get_data_iterator</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">train_file</span><span class="p">,</span> \
         <span class="nb">open</span><span class="p">(</span><span class="n">dev_path</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">dev_file</span><span class="p">,</span> \
         <span class="nb">open</span><span class="p">(</span><span class="n">test_path</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">test_file</span><span class="p">:</span>
        
        <span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">data_iter</span><span class="p">):</span>
            <span class="n">radio</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">radio</span> <span class="o">&lt;</span> <span class="mf">0.8</span><span class="p">:</span>
                <span class="n">train_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span> <span class="o">+</span> <span class="s2">&#34;</span><span class="se">\t</span><span class="s2">&#34;</span> <span class="o">+</span> <span class="n">label</span> <span class="o">+</span> <span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">radio</span> <span class="o">&lt;</span> <span class="mf">0.9</span><span class="p">:</span>
                <span class="n">dev_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span> <span class="o">+</span> <span class="s2">&#34;</span><span class="se">\t</span><span class="s2">&#34;</span> <span class="o">+</span> <span class="n">label</span> <span class="o">+</span> <span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">test_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span> <span class="o">+</span> <span class="s2">&#34;</span><span class="se">\t</span><span class="s2">&#34;</span> <span class="o">+</span> <span class="n">label</span> <span class="o">+</span> <span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="c1"># build_dataset(data_path, r&#34;data/keras_bert_train.txt&#34;, r&#34;data/keras_bert_dev.txt&#34;, r&#34;data/keras_bert_test.txt&#34;)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>

<span class="gh"># 获取数据集样本个数
</span><span class="gh"></span>

<span class="s">```python
</span><span class="s"></span><span class="k">def</span> <span class="nf">get_sample_num</span><span class="p">(</span><span class="n">data_path</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">data_file</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">data_file</span><span class="p">):</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">count</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">train_sample_count</span> <span class="o">=</span> <span class="n">get_sample_num</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_train.txt&#34;</span><span class="p">)</span>
<span class="s">```</span>

    668858it [00:09, 71362.95it/s]



<span class="s">```python
</span><span class="s"></span><span class="n">dev_sample_count</span> <span class="o">=</span> <span class="n">get_sample_num</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_dev.txt&#34;</span><span class="p">)</span>
<span class="s">```</span>

    83721it [00:01, 72723.97it/s]



<span class="s">```python
</span><span class="s"></span><span class="n">test_sample_count</span> <span class="o">=</span> <span class="n">get_sample_num</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_test.txt&#34;</span><span class="p">)</span>
<span class="s">```</span>

    83496it [00:01, 72445.72it/s]



<span class="s">```python
</span><span class="s"></span><span class="n">train_sample_count</span><span class="p">,</span> <span class="n">dev_sample_count</span><span class="p">,</span> <span class="n">test_sample_count</span>
<span class="s">```</span>




    (668858, 83721, 83496)




<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>

<span class="gh"># 构建数据迭代器
</span><span class="gh"></span>

<span class="s">```python
</span><span class="s"></span><span class="k">def</span> <span class="nf">get_text_iterator</span><span class="p">(</span><span class="n">data_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">data_file</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">data_file</span><span class="p">:</span>
            <span class="n">data_split</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_split</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                <span class="k">continue</span>
            <span class="k">yield</span> <span class="n">data_split</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">data_split</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">it</span> <span class="o">=</span> <span class="n">get_text_iterator</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_train.txt&#34;</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="nb">next</span><span class="p">(</span><span class="n">it</span><span class="p">)</span>
<span class="s">```</span>




    (&#39;竞彩解析：日本美国争冠死磕 两巴相逢必有生死。周日受注赛事，女足世界杯决赛、美洲杯两场1/4决赛毫无疑问是全世界球迷和彩民关注的焦点。本届女足世界杯的最大黑马日本队能否一黑到底，创造亚洲奇迹？女子足坛霸主美国队能否再次“灭黑”成功，成就三冠伟业？巴西、巴拉圭冤家路窄，谁又能笑到最后？诸多谜底，在周一凌晨就会揭晓。日本美国争冠死磕。本届女足世界杯，是颠覆与反颠覆之争。夺冠大热门东道主德国队1/4决赛被日本队加时赛一球而“黑”，另一个夺冠大热门瑞典队则在半决赛被日本队3:1彻底打垮。而美国队则捍卫着女足豪强的尊严，在1/4决赛，她们与巴西女足苦战至点球大战，最终以5:3淘汰这支迅速崛起的黑马球队，而在半决赛，她们更是3:1大胜欧洲黑马法国队。美日两队此次世界杯进程惊人相似，小组赛前两轮全胜，最后一轮输球，1/4决赛同样与对手90分钟内战成平局，半决赛竟同样3:1大胜对手。此次决战，无论是日本还是美国队夺冠，均将创造女足世界杯新的历史。两巴相逢必有生死。本届美洲杯，让人大跌眼镜的事情太多。巴西、巴拉圭冤家路窄似乎更具传奇色彩。两队小组赛同分在B组，原本两个出线大热门，却双双在前两轮小组赛战平，两队直接交锋就是2:2平局，结果双双面临出局危险。最后一轮，巴西队在下半场终于发威，4:2大胜厄瓜多尔后来居上以小组第一出线，而巴拉圭最后一战还是3:3战平委内瑞拉获得小组第三，侥幸凭借净胜球优势挤掉A组第三名的哥斯达黎加，获得一个八强席位。在小组赛，巴西队是在最后时刻才逼平了巴拉圭，他们的好运气会在淘汰赛再显神威吗？巴拉圭此前3轮小组赛似乎都缺乏运气，此番又会否被幸运之神补偿一下呢？。另一场美洲杯1/4决赛，智利队在C组小组赛2胜1平以小组头名晋级八强；而委内瑞拉在B组是最不被看好的球队，但竟然在与巴西、巴拉圭同组的情况下，前两轮就奠定了小组出线权，他们小组3战1胜2平保持不败战绩，而入球数跟智利一样都是4球，只是失球数比智利多了1个。但既然他们面对强大的巴西都能保持球门不失，此番再创佳绩也不足为怪。&#39;,
     &#39;彩票&#39;)




<span class="s">```python
</span><span class="s"></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">bert_path</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="k">def</span> <span class="nf">get_keras_bert_iterator</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">data_iter</span> <span class="o">=</span> <span class="n">get_text_iterator</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">category</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> 
                                         <span class="n">max_length</span><span class="o">=</span><span class="n">text_max_length</span><span class="p">,</span> 
                                         <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                                         <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;max_length&#39;</span><span class="p">,</span> 
                                         <span class="n">truncation_strategy</span><span class="o">=</span><span class="s1">&#39;only_first&#39;</span><span class="p">,</span> 
<span class="c1">#                                          return_tensors=&#39;tf&#39;</span>
                                        <span class="p">)</span>
            <span class="k">yield</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">[</span><span class="n">category</span><span class="p">]</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">it</span> <span class="o">=</span> <span class="n">get_keras_bert_iterator</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_train.txt&#34;</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="c1"># next(it)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>

<span class="gh"># 构建批次数据迭代器
</span><span class="gh"></span>

<span class="s">```python
</span><span class="s"></span><span class="k">def</span> <span class="nf">batch_iter</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;生成批次数据&#34;&#34;&#34;</span>
    <span class="n">keras_bert_iter</span> <span class="o">=</span> <span class="n">get_keras_bert_iterator</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">data_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">keras_bert_iter</span><span class="p">)</span>
            <span class="n">data_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
            <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">data_list</span><span class="p">)</span>
        
        <span class="n">indices_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">label_index_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">data_list</span><span class="p">:</span>
            <span class="n">indices</span><span class="p">,</span> <span class="n">label_index</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">indices_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
            <span class="n">label_index_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label_index</span><span class="p">)</span>

        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">indices_list</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">label_index_list</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">it</span> <span class="o">=</span> <span class="n">batch_iter</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_train.txt&#34;</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="c1"># next(it)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">it</span> <span class="o">=</span> <span class="n">batch_iter</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_train.txt&#34;</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="nb">next</span><span class="p">(</span><span class="n">it</span><span class="p">)</span>
<span class="s">```</span>

    /home/zsd-server/miniconda3/envs/my/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2162: FutureWarning: The <span class="sb">`truncation_strategy`</span> argument is deprecated and will be removed in a future version, use <span class="sb">`truncation=True`</span> to truncate examples to a max length. You can give a specific length with <span class="sb">`max_length`</span> (e.g. <span class="sb">`max_length=45`</span>) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among <span class="sb">`truncation=&#39;only_first&#39;`</span> (will only truncate the first sentence in the pairs) <span class="sb">`truncation=&#39;only_second&#39;`</span> (will only truncate the second sentence in the pairs) or <span class="sb">`truncation=&#39;longest_first&#39;`</span> (will iteratively remove tokens from the longest sentence in the pairs).
      warnings.warn(





    (array([[ 101, 4993, 2506, ...,  131,  123,  102],
            [ 101, 2506, 3696, ..., 1139,  125,  102]]),
     array([0, 0]))




<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>

<span class="gh"># 定义base模型
</span><span class="gh"></span>

<span class="s">```python
</span><span class="s"></span><span class="c1"># !transformers-cli convert --model_type bert \</span>
<span class="c1">#   --tf_checkpoint chinese_L-12_H-768_A-12/bert_model.ckpt \</span>
<span class="c1">#   --config chinese_L-12_H-768_A-12/bert_config.json \</span>
<span class="c1">#   --pytorch_dump_output chinese_L-12_H-768_A-12/pytorch_model.bin</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="c1"># bert_model = TFBertForPreTraining.from_pretrained(&#34;./chinese_L-12_H-768_A-12/&#34;, from_pt=True)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="c1"># # it = get_keras_bert_iterator(r&#34;data/keras_bert_train.txt&#34;, cat_to_id, tokenizer)</span>
<span class="c1"># it = batch_iter(r&#34;data/keras_bert_train.txt&#34;, cat_to_id, tokenizer, batch_size=1)</span>
<span class="c1"># out = bert_model(next(it)[0])</span>
<span class="c1"># out[0]</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">label_list</span><span class="p">):</span>
    <span class="n">K</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
    
    <span class="n">bert_model</span> <span class="o">=</span> <span class="n">TFBertForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">bert_path</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
 
    <span class="n">input_indices</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">)</span>
 
    <span class="n">bert_output</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span><span class="n">input_indices</span><span class="p">)</span>
    <span class="n">projection_logits</span> <span class="o">=</span> <span class="n">bert_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">bert_cls</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])(</span><span class="n">projection_logits</span><span class="p">)</span> <span class="c1"># 取出[CLS]对应的向量用来做分类</span>
    
    <span class="n">dropout</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">bert_cls</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label_list</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">dropout</span><span class="p">)</span>
 
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">input_indices</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span>
                  <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">),</span>    <span class="c1">#用足够小的学习率</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">model</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">early_stopping</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>   <span class="c1">#早停法，防止过拟合</span>
<span class="n">plateau</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&#34;val_accuracy&#34;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1">#当评价指标不在提升时，减少学习率</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s1">&#39;trained_model/keras_bert_THUCNews.hdf5&#39;</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="n">save_weights_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1">#保存最好的模型</span>
<span class="s">```</span>

<span class="gu">## 模型训练
</span><span class="gu"></span>

<span class="s">```python
</span><span class="s"></span><span class="k">def</span> <span class="nf">get_step</span><span class="p">(</span><span class="n">sample_count</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">sample_count</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="k">if</span> <span class="n">sample_count</span> <span class="o">%</span> <span class="n">batch_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">step</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">train_step</span> <span class="o">=</span> <span class="n">get_step</span><span class="p">(</span><span class="n">train_sample_count</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">dev_step</span> <span class="o">=</span> <span class="n">get_step</span><span class="p">(</span><span class="n">dev_sample_count</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="n">train_dataset_iterator</span> <span class="o">=</span> <span class="n">batch_iter</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_train.txt&#34;</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">dev_dataset_iterator</span> <span class="o">=</span> <span class="n">batch_iter</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_dev.txt&#34;</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="n">categories</span><span class="p">)</span>

<span class="c1">#模型训练</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">train_dataset_iterator</span><span class="p">,</span>
    <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="n">dev_dataset_iterator</span><span class="p">,</span>
    <span class="n">validation_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">early_stopping</span><span class="p">,</span> <span class="n">plateau</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">],</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>
<span class="s">```</span>

    Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForPreTraining: [&#39;bert.embeddings.position_ids&#39;, &#39;cls.predictions.decoder.bias&#39;]
    <span class="k">-</span> This IS expected if you are initializing TFBertForPreTraining from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
    <span class="k">-</span> This IS NOT expected if you are initializing TFBertForPreTraining from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
    All the weights of TFBertForPreTraining were initialized from the PyTorch model.
    If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.


    WARNING:tensorflow:AutoGraph could not transform &lt;bound method Socket.send of &lt;zmq.sugar.socket.Socket object at 0x7f3e4d7e2340&gt;&gt; and will run it as-is.
    Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, <span class="sb">`export AUTOGRAPH_VERBOSITY=10`</span>) and attach the full output.
    Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method
    To silence this warning, decorate the function with <span class="ni">@tf</span>.autograph.experimental.do_not_convert
    WARNING: AutoGraph could not transform &lt;bound method Socket.send of &lt;zmq.sugar.socket.Socket object at 0x7f3e4d7e2340&gt;&gt; and will run it as-is.
    Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, <span class="sb">`export AUTOGRAPH_VERBOSITY=10`</span>) and attach the full output.
    Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method
    To silence this warning, decorate the function with <span class="ni">@tf</span>.autograph.experimental.do_not_convert


    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    /home/zsd-server/miniconda3/envs/my/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2162: FutureWarning: The <span class="sb">`truncation_strategy`</span> argument is deprecated and will be removed in a future version, use <span class="sb">`truncation=True`</span> to truncate examples to a max length. You can give a specific length with <span class="sb">`max_length`</span> (e.g. <span class="sb">`max_length=45`</span>) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among <span class="sb">`truncation=&#39;only_first&#39;`</span> (will only truncate the first sentence in the pairs) <span class="sb">`truncation=&#39;only_second&#39;`</span> (will only truncate the second sentence in the pairs) or <span class="sb">`truncation=&#39;longest_first&#39;`</span> (will iteratively remove tokens from the longest sentence in the pairs).
      warnings.warn(
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.


    Model: &#34;functional_1&#34;
    <span class="gs">_________________________________________________________________</span>
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_1 (InputLayer)         [(None, None)]            0         
    <span class="gs">_________________________________________________________________</span>
    tf_bert_for_pre_training (TF TFBertForPreTrainingOutpu 102882442 
    <span class="gs">_________________________________________________________________</span>
    lambda (Lambda)              (None, 21128)             0         
    <span class="gs">_________________________________________________________________</span>
    dropout_37 (Dropout)         (None, 21128)             0         
    <span class="gs">_________________________________________________________________</span>
    dense (Dense)                (None, 14)                295806    
    =================================================================
    Total params: 103,178,248
    Trainable params: 103,178,248
    Non-trainable params: 0
    <span class="gs">_________________________________________________________________</span>
    None
    Epoch 1/5
    WARNING:tensorflow:Gradients do not exist for variables [&#39;tf_bert_for_pre_training/bert/pooler/dense/kernel:0&#39;, &#39;tf_bert_for_pre_training/bert/pooler/dense/bias:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/kernel:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/bias:0&#39;] when minimizing the loss.
    WARNING:tensorflow:Gradients do not exist for variables [&#39;tf_bert_for_pre_training/bert/pooler/dense/kernel:0&#39;, &#39;tf_bert_for_pre_training/bert/pooler/dense/bias:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/kernel:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/bias:0&#39;] when minimizing the loss.


    /home/zsd-server/miniconda3/envs/my/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
      warnings.warn(
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.


    WARNING:tensorflow:Gradients do not exist for variables [&#39;tf_bert_for_pre_training/bert/pooler/dense/kernel:0&#39;, &#39;tf_bert_for_pre_training/bert/pooler/dense/bias:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/kernel:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/bias:0&#39;] when minimizing the loss.
    WARNING:tensorflow:Gradients do not exist for variables [&#39;tf_bert_for_pre_training/bert/pooler/dense/kernel:0&#39;, &#39;tf_bert_for_pre_training/bert/pooler/dense/bias:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/kernel:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/bias:0&#39;] when minimizing the loss.
    10/10 [==============================] - ETA: 0s - loss: 13.3638 - accuracy: 0.1000    

    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.


    
    Epoch 00001: val_loss improved from -inf to 0.80593, saving model to trained_model/keras_bert_THUCNews.hdf5
    10/10 [==============================] - 89s 9s/step - loss: 13.3638 - accuracy: 0.1000 - val_loss: 0.8059 - val_accuracy: 1.0000
    Epoch 2/5
    10/10 [==============================] - ETA: 0s - loss: 2.2765 - accuracy: 0.3500
    Epoch 00002: val_loss did not improve from 0.80593
    10/10 [==============================] - 87s 9s/step - loss: 2.2765 - accuracy: 0.3500 - val_loss: 0.6079 - val_accuracy: 0.7500
    Epoch 3/5
    10/10 [==============================] - ETA: 0s - loss: 0.3474 - accuracy: 0.8500
    Epoch 00003: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.
    
    Epoch 00003: val_loss did not improve from 0.80593
    10/10 [==============================] - 88s 9s/step - loss: 0.3474 - accuracy: 0.8500 - val_loss: 2.5302e-05 - val_accuracy: 1.0000
    Epoch 4/5
    10/10 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000
    Epoch 00004: val_loss did not improve from 0.80593
    10/10 [==============================] - 86s 9s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 6.5565e-07 - val_accuracy: 1.0000
    Epoch 5/5
    10/10 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 1.0000
    Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.
    
    Epoch 00005: val_loss did not improve from 0.80593
    10/10 [==============================] - 86s 9s/step - loss: 0.0372 - accuracy: 1.0000 - val_loss: 5.9605e-07 - val_accuracy: 1.0000





    &lt;tensorflow.python.keras.callbacks.History at 0x7f3dc062d820&gt;




<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>

<span class="gh"># 多输出模型
</span><span class="gh"></span>
<span class="gu">## 构建数据迭代器
</span><span class="gu"></span>

<span class="s">```python
</span><span class="s"></span><span class="n">second_label_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="k">def</span> <span class="nf">get_keras_bert_iterator</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">second_label_list</span><span class="p">):</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">data_iter</span> <span class="o">=</span> <span class="n">get_text_iterator</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">text</span><span class="p">,</span> <span class="n">category</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">:</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> 
                                         <span class="n">max_length</span><span class="o">=</span><span class="n">text_max_length</span><span class="p">,</span> 
                                         <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                                         <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;max_length&#39;</span><span class="p">,</span> 
                                         <span class="n">truncation_strategy</span><span class="o">=</span><span class="s1">&#39;only_first&#39;</span><span class="p">,</span> 
<span class="c1">#                                          return_tensors=&#39;tf&#39;</span>
                                        <span class="p">)</span>
            <span class="k">yield</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">[</span><span class="n">category</span><span class="p">],</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">second_label_list</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">it</span> <span class="o">=</span> <span class="n">get_keras_bert_iterator</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_train.txt&#34;</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">second_label_list</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="c1"># next(it)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="k">def</span> <span class="nf">batch_iter</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">second_label_list</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;生成批次数据&#34;&#34;&#34;</span>
    <span class="n">keras_bert_iter</span> <span class="o">=</span> <span class="n">get_keras_bert_iterator</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">second_label_list</span><span class="p">)</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">data_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">keras_bert_iter</span><span class="p">)</span>
            <span class="n">data_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
            <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">data_list</span><span class="p">)</span>
        
        <span class="n">indices_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">label_index_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">second_label_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">data_list</span><span class="p">:</span>
            <span class="n">indices</span><span class="p">,</span> <span class="n">label_index</span><span class="p">,</span> <span class="n">second_label</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">indices_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
            <span class="n">label_index_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label_index</span><span class="p">)</span>
            <span class="n">second_label_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">second_label</span><span class="p">)</span>

        <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">indices_list</span><span class="p">),</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">label_index_list</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">second_label_list</span><span class="p">)]</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">it</span> <span class="o">=</span> <span class="n">batch_iter</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_train.txt&#34;</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">second_label_list</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="nb">next</span><span class="p">(</span><span class="n">it</span><span class="p">)</span>
<span class="s">```</span>




    (array([[ 101, 2506, 3696, ..., 1139,  125,  102],
            [ 101, 4993, 2506, ...,  131,  123,  102]]),
     [array([0, 0]), array([1, 2])])



<span class="gu">## 定义模型
</span><span class="gu"></span>

<span class="s">```python
</span><span class="s"></span><span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">label_list</span><span class="p">,</span> <span class="n">second_label_list</span><span class="p">):</span>
    <span class="n">K</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
    
    <span class="n">bert_model</span> <span class="o">=</span> <span class="n">TFBertForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">bert_path</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
 
    <span class="n">input_indices</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">)</span>
 
    <span class="n">bert_output</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span><span class="n">input_indices</span><span class="p">)</span>
    <span class="n">projection_logits</span> <span class="o">=</span> <span class="n">bert_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">bert_cls</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])(</span><span class="n">projection_logits</span><span class="p">)</span> <span class="c1"># 取出[CLS]对应的向量用来做分类</span>
    
    <span class="n">dropout</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">bert_cls</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label_list</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="n">dropout_second</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">bert_cls</span><span class="p">)</span>
    <span class="n">output_second</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">second_label_list</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">dropout_second</span><span class="p">)</span>
 
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">input_indices</span><span class="p">,</span> <span class="p">[</span><span class="n">output</span><span class="p">,</span> <span class="n">output_second</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span>
                  <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">),</span>    <span class="c1">#用足够小的学习率</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">model</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">early_stopping</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>   <span class="c1">#早停法，防止过拟合</span>
<span class="n">plateau</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&#34;val_loss&#34;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1">#当评价指标不在提升时，减少学习率</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s1">&#39;trained_model/muilt_keras_bert_THUCNews.hdf5&#39;</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="n">save_weights_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1">#保存最好的模型</span>
<span class="s">```</span>

<span class="gu">## 模型训练
</span><span class="gu"></span>

<span class="s">```python
</span><span class="s"></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">train_step</span> <span class="o">=</span> <span class="n">get_step</span><span class="p">(</span><span class="n">train_sample_count</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">dev_step</span> <span class="o">=</span> <span class="n">get_step</span><span class="p">(</span><span class="n">dev_sample_count</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="n">train_dataset_iterator</span> <span class="o">=</span> <span class="n">batch_iter</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_train.txt&#34;</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">second_label_list</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">dev_dataset_iterator</span> <span class="o">=</span> <span class="n">batch_iter</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_dev.txt&#34;</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">second_label_list</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="n">categories</span><span class="p">,</span> <span class="n">second_label_list</span><span class="p">)</span>

<span class="c1">#模型训练</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">train_dataset_iterator</span><span class="p">,</span>
    <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="n">dev_dataset_iterator</span><span class="p">,</span>
    <span class="n">validation_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">early_stopping</span><span class="p">,</span> <span class="n">plateau</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">],</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>
<span class="s">```</span>

    Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForPreTraining: [&#39;bert.embeddings.position_ids&#39;, &#39;cls.predictions.decoder.bias&#39;]
    <span class="k">-</span> This IS expected if you are initializing TFBertForPreTraining from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
    <span class="k">-</span> This IS NOT expected if you are initializing TFBertForPreTraining from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
    All the weights of TFBertForPreTraining were initialized from the PyTorch model.
    If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.


    Model: &#34;functional_1&#34;
    <span class="gs">__________________________________________________________________________________________________</span>
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    input_1 (InputLayer)            [(None, None)]       0                                            
    <span class="gs">__________________________________________________________________________________________________</span>
    tf_bert_for_pre_training (TFBer TFBertForPreTraining 102882442   input_1[0][0]                    
    <span class="gs">__________________________________________________________________________________________________</span>
    lambda (Lambda)                 (None, 21128)        0           tf_bert_for_pre_training[0][0]   
    <span class="gs">__________________________________________________________________________________________________</span>
    dropout_37 (Dropout)            (None, 21128)        0           lambda[0][0]                     
    <span class="gs">__________________________________________________________________________________________________</span>
    dropout_38 (Dropout)            (None, 21128)        0           lambda[0][0]                     
    <span class="gs">__________________________________________________________________________________________________</span>
    dense (Dense)                   (None, 14)           295806      dropout_37[0][0]                 
    <span class="gs">__________________________________________________________________________________________________</span>
    dense_1 (Dense)                 (None, 3)            63387       dropout_38[0][0]                 
    ==================================================================================================
    Total params: 103,241,635
    Trainable params: 103,241,635
    Non-trainable params: 0
    <span class="gs">__________________________________________________________________________________________________</span>
    None
    Epoch 1/5
    WARNING:tensorflow:Gradients do not exist for variables [&#39;tf_bert_for_pre_training/bert/pooler/dense/kernel:0&#39;, &#39;tf_bert_for_pre_training/bert/pooler/dense/bias:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/kernel:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/bias:0&#39;] when minimizing the loss.
    WARNING:tensorflow:Gradients do not exist for variables [&#39;tf_bert_for_pre_training/bert/pooler/dense/kernel:0&#39;, &#39;tf_bert_for_pre_training/bert/pooler/dense/bias:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/kernel:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/bias:0&#39;] when minimizing the loss.


    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.


    WARNING:tensorflow:Gradients do not exist for variables [&#39;tf_bert_for_pre_training/bert/pooler/dense/kernel:0&#39;, &#39;tf_bert_for_pre_training/bert/pooler/dense/bias:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/kernel:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/bias:0&#39;] when minimizing the loss.
    WARNING:tensorflow:Gradients do not exist for variables [&#39;tf_bert_for_pre_training/bert/pooler/dense/kernel:0&#39;, &#39;tf_bert_for_pre_training/bert/pooler/dense/bias:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/kernel:0&#39;, &#39;tf_bert_for_pre_training/nsp___cls/seq_relationship/bias:0&#39;] when minimizing the loss.
    10/10 [==============================] - ETA: 0s - loss: 13.2166 - dense_loss: 7.6030 - dense_1_loss: 5.6137 - dense_accuracy: 0.3000 - dense_1_accuracy: 0.3000 

    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.


    
    Epoch 00001: val_loss improved from -inf to 1.83895, saving model to trained_model/muilt_keras_bert_THUCNews.hdf5
    10/10 [==============================] - 92s 9s/step - loss: 13.2166 - dense_loss: 7.6030 - dense_1_loss: 5.6137 - dense_accuracy: 0.3000 - dense_1_accuracy: 0.3000 - val_loss: 1.8389 - val_dense_loss: 0.0022 - val_dense_1_loss: 1.8367 - val_dense_accuracy: 1.0000 - val_dense_1_accuracy: 0.5000
    Epoch 2/5
    10/10 [==============================] - ETA: 0s - loss: 3.4398 - dense_loss: 0.6309 - dense_1_loss: 2.8089 - dense_accuracy: 0.8000 - dense_1_accuracy: 0.3500
    Epoch 00002: val_loss improved from 1.83895 to 2.41962, saving model to trained_model/muilt_keras_bert_THUCNews.hdf5
    10/10 [==============================] - 84s 8s/step - loss: 3.4398 - dense_loss: 0.6309 - dense_1_loss: 2.8089 - dense_accuracy: 0.8000 - dense_1_accuracy: 0.3500 - val_loss: 2.4196 - val_dense_loss: 3.2663e-04 - val_dense_1_loss: 2.4193 - val_dense_accuracy: 1.0000 - val_dense_1_accuracy: 0.2500
    Epoch 3/5
    10/10 [==============================] - ETA: 0s - loss: 2.4220 - dense_loss: 0.0156 - dense_1_loss: 2.4063 - dense_accuracy: 1.0000 - dense_1_accuracy: 0.3000
    Epoch 00003: val_loss did not improve from 2.41962
    10/10 [==============================] - 86s 9s/step - loss: 2.4220 - dense_loss: 0.0156 - dense_1_loss: 2.4063 - dense_accuracy: 1.0000 - dense_1_accuracy: 0.3000 - val_loss: 1.3294 - val_dense_loss: 2.6792e-05 - val_dense_1_loss: 1.3294 - val_dense_accuracy: 1.0000 - val_dense_1_accuracy: 0.2500
    Epoch 4/5
    10/10 [==============================] - ETA: 0s - loss: 2.2683 - dense_loss: 0.0068 - dense_1_loss: 2.2615 - dense_accuracy: 1.0000 - dense_1_accuracy: 0.3500
    Epoch 00004: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.
    
    Epoch 00004: val_loss did not improve from 2.41962
    10/10 [==============================] - 89s 9s/step - loss: 2.2683 - dense_loss: 0.0068 - dense_1_loss: 2.2615 - dense_accuracy: 1.0000 - dense_1_accuracy: 0.3500 - val_loss: 1.1061 - val_dense_loss: 0.0025 - val_dense_1_loss: 1.1036 - val_dense_accuracy: 1.0000 - val_dense_1_accuracy: 0.2500
    Epoch 5/5
    10/10 [==============================] - ETA: 0s - loss: 3.7829 - dense_loss: 0.6761 - dense_1_loss: 3.1068 - dense_accuracy: 0.9500 - dense_1_accuracy: 0.3000
    Epoch 00005: val_loss did not improve from 2.41962
    10/10 [==============================] - 89s 9s/step - loss: 3.7829 - dense_loss: 0.6761 - dense_1_loss: 3.1068 - dense_accuracy: 0.9500 - dense_1_accuracy: 0.3000 - val_loss: 1.3932 - val_dense_loss: 0.0022 - val_dense_1_loss: 1.3909 - val_dense_accuracy: 1.0000 - val_dense_1_accuracy: 0.0000e+00





    &lt;tensorflow.python.keras.callbacks.History at 0x7f3d7ce29cd0&gt;




<span class="s">```python
</span><span class="s"></span><span class="n">dev_dataset_iterator</span> <span class="o">=</span> <span class="n">batch_iter</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_dev.txt&#34;</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">second_label_list</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate_generator</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="n">dev_dataset_iterator</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="s">```</span>

    WARNING:tensorflow:From &lt;ipython-input-47-b8b146ad62ac&gt;:2: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
    Instructions for updating:
    Please use Model.evaluate, which supports generators.





    [1.1637728214263916, 0.003070184262469411, 1.1607024669647217, 1.0, 0.0]




<span class="s">```python
</span><span class="s"></span><span class="n">model</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s2">&#34;trained_model/muilt_keras_bert_THUCNews_final.weights&#34;</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&#34;trained_model/muilt_keras_bert_THUCNews_final.model&#34;</span><span class="p">)</span>
<span class="s">```</span>

    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.


    WARNING:tensorflow:From /home/zsd-server/miniconda3/envs/my/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
    Instructions for updating:
    This property should not be used in TensorFlow 2.0, as updates are applied automatically.
    WARNING:tensorflow:From /home/zsd-server/miniconda3/envs/my/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
    Instructions for updating:
    This property should not be used in TensorFlow 2.0, as updates are applied automatically.


    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.


    INFO:tensorflow:Assets written to: trained_model/muilt_keras_bert_THUCNews_final.model/assets



<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>

<span class="gh"># 模型加载及测试
</span><span class="gh"></span>
<span class="gu">## load_weights
</span><span class="gu"></span>

<span class="s">```python
</span><span class="s"></span><span class="n">model_test</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="n">categories</span><span class="p">,</span> <span class="n">second_label_list</span><span class="p">)</span>
<span class="s">```</span>

    Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForPreTraining: [&#39;bert.embeddings.position_ids&#39;, &#39;cls.predictions.decoder.bias&#39;]
    <span class="k">-</span> This IS expected if you are initializing TFBertForPreTraining from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
    <span class="k">-</span> This IS NOT expected if you are initializing TFBertForPreTraining from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
    All the weights of TFBertForPreTraining were initialized from the PyTorch model.
    If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.


    Model: &#34;functional_1&#34;
    <span class="gs">__________________________________________________________________________________________________</span>
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    input_1 (InputLayer)            [(None, None)]       0                                            
    <span class="gs">__________________________________________________________________________________________________</span>
    tf_bert_for_pre_training (TFBer TFBertForPreTraining 102882442   input_1[0][0]                    
    <span class="gs">__________________________________________________________________________________________________</span>
    lambda (Lambda)                 (None, 21128)        0           tf_bert_for_pre_training[0][0]   
    <span class="gs">__________________________________________________________________________________________________</span>
    dropout_37 (Dropout)            (None, 21128)        0           lambda[0][0]                     
    <span class="gs">__________________________________________________________________________________________________</span>
    dropout_38 (Dropout)            (None, 21128)        0           lambda[0][0]                     
    <span class="gs">__________________________________________________________________________________________________</span>
    dense (Dense)                   (None, 14)           295806      dropout_37[0][0]                 
    <span class="gs">__________________________________________________________________________________________________</span>
    dense_1 (Dense)                 (None, 3)            63387       dropout_38[0][0]                 
    ==================================================================================================
    Total params: 103,241,635
    Trainable params: 103,241,635
    Non-trainable params: 0
    <span class="gs">__________________________________________________________________________________________________</span>
    None



<span class="s">```python
</span><span class="s"></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">dev_dataset_iterator</span> <span class="o">=</span> <span class="n">batch_iter</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_dev.txt&#34;</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">second_label_list</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">model_test</span><span class="o">.</span><span class="n">evaluate_generator</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="n">dev_dataset_iterator</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="s">```</span>

    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.
    The parameters <span class="sb">`output_attentions`</span>, <span class="sb">`output_hidden_states`</span> and <span class="sb">`use_cache`</span> cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: <span class="sb">`config=XConfig.from_pretrained(&#39;name&#39;, output_attentions=True)`</span>).
    The parameter <span class="sb">`return_dict`</span> cannot be set in graph mode and will always be set to <span class="sb">`True`</span>.





    [31.814546585083008, 23.794151306152344, 8.02039623260498, 0.0, 0.25]




<span class="s">```python
</span><span class="s"></span><span class="n">model_test</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s2">&#34;trained_model/muilt_keras_bert_THUCNews_final.weights&#34;</span><span class="p">)</span>
<span class="s">```</span>




    &lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3d53816700&gt;




<span class="s">```python
</span><span class="s"></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">dev_dataset_iterator</span> <span class="o">=</span> <span class="n">batch_iter</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;data/keras_bert_dev.txt&#34;</span><span class="p">,</span> <span class="n">cat_to_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">second_label_list</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">model_test</span><span class="o">.</span><span class="n">evaluate_generator</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="n">dev_dataset_iterator</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="s">```</span>




    [0.815148115158081, 0.003070184262469411, 0.8120779395103455, 1.0, 0.25]



<span class="gu">## load_model
</span><span class="gu"></span>

<span class="s">```python
</span><span class="s"></span><span class="n">model_test</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s2">&#34;trained_model/muilt_keras_bert_THUCNews_final.model&#34;</span><span class="p">)</span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>


<span class="s">```python
</span><span class="s"></span>
<span class="s">```</span>

</code></pre></td></tr></table>
</div>
</div>
    </div>

    
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">赞赏支持</label>
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="/images/money_weixin_20200719212002.png">
        <span>微信打赏</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="/images/alipay_20200801211208.png">
        <span>支付宝打赏</span>
      </label>
  </div>
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
          <a href="/tags/%E5%A4%9A%E8%BE%93%E5%87%BA/">多输出</a>
          <a href="/tags/%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB/">参数共享</a>
          <a href="/tags/%E6%A8%A1%E5%9E%8B/">模型</a>
          <a href="/tags/keras/">keras</a>
          <a href="/tags/bert/">bert</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/custom_loss_to_switch_network/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">用自定义损失函数实现选择启用不同子网络</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/tree_model/">
            <span class="next-text nav-default">树模型</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        <div id="gitalk-container"></div>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js" crossorigin="anonymous"></script>
    <script type="text/javascript">
      var gitalk = new Gitalk({
        id: '2021-03-27 00:57:58 \x2b0800 CST',
        title: '用transformers实现多输出、参数共享的bert模型',
        clientID: '5ea75f603117948d8d37',
        clientSecret: '26c617c6bce9a975c2a65a68f1ca2a2cc7dde587',
        repo: 'blog',
        owner: 'zhangsheng377',
        admin: ['zhangsheng377'],
        body: decodeURI(location.href)
      });
      gitalk.render('gitalk-container');
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/gitalk/gitalk">comments powered by gitalk.</a></noscript>

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:435878393@qq.com" class="iconfont icon-email" title="email"></a>
      <a href="https://www.linkedin.com/in/zhangshengdong/" class="iconfont icon-linkedin" title="linkedin"></a>
      <a href="https://github.com/zhangsheng377/" class="iconfont icon-github" title="github"></a>
  <a href="https://www.zhangshengdong.com/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> 本站总访问量 <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次 </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> 本站总访客数 <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 人 </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2019 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span><a href="https://beian.miit.gov.cn/" target="_blank">苏ICP备15009593号-1</a></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        tags: 'ams',
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>

<script id="baidu_analytics">
  var _hmt = _hmt || [];
  (function() {
    if (window.location.hostname === 'localhost') return;
    var hm = document.createElement("script"); hm.async = true;
    hm.src = "https://hm.baidu.com/hm.js?c602db2501643f661d9789f9e9707386";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>






</body>
</html>
